{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiwei\\AppData\\Local\\Temp\\ipykernel_253500\\860887196.py:68: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(mean_value, inplace=True)\n",
      "C:\\Users\\xiwei\\AppData\\Local\\Temp\\ipykernel_253500\\860887196.py:72: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(mode_value, inplace=True)\n",
      "C:\\Users\\xiwei\\AppData\\Local\\Temp\\ipykernel_253500\\860887196.py:68: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(mean_value, inplace=True)\n",
      "C:\\Users\\xiwei\\AppData\\Local\\Temp\\ipykernel_253500\\860887196.py:72: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(mode_value, inplace=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'aee383561c0019dc01552bfa5263af8etbuc7'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_253500\\860887196.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'outcome'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'outcome'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'number'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'object'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[0mclf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gini'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m \u001b[0mclf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RF准确率：'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\xiwei\\miniconda3\\envs\\data_science\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1471\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1472\u001b[0m                 \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1473\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1474\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\xiwei\\miniconda3\\envs\\data_science\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    364\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\xiwei\\miniconda3\\envs\\data_science\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    646\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"estimator\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m                     \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\xiwei\\miniconda3\\envs\\data_science\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1259\u001b[0m         raise ValueError(\n\u001b[0;32m   1260\u001b[0m             \u001b[1;33mf\"\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mestimator_name\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m requires y to be passed, but the target y is None\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1263\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m   1264\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m         \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\xiwei\\miniconda3\\envs\\data_science\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    994\u001b[0m                         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 998\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    999\u001b[0m                 raise ValueError(\n\u001b[0;32m   1000\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                 \u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\xiwei\\miniconda3\\envs\\data_science\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\xiwei\\miniconda3\\envs\\data_science\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2148\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2150\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2151\u001b[0m         if (\n\u001b[0;32m   2152\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2153\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'aee383561c0019dc01552bfa5263af8etbuc7'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 读取数据\n",
    "trade_df = pd.read_csv('./trade.csv')\n",
    "train_df = pd.read_csv('./user_train.csv')\n",
    "test_df = pd.read_csv('./user_test.csv')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "'''\n",
    "# 使用 Min-Max 标准化\n",
    "min_max_scaler = MinMaxScaler()\n",
    "trade_df['time_normalized'] = min_max_scaler.fit_transform(trade_df[['time']])\n",
    "trade_df\n",
    "\n",
    "'''\n",
    "# 使用 Z-score 标准化\n",
    "z_score_scaler = StandardScaler()\n",
    "trade_df['time_normalized'] = z_score_scaler.fit_transform(trade_df[['time']])\n",
    "\n",
    "def trade_data_processing(trade_df):\n",
    "    trade_count = trade_df.groupby('bidder_id').size().reset_index(name='trade_count')\n",
    "    trade_count = trade_count.reset_index(drop=True)\n",
    "\n",
    "    device_counts = trade_df.groupby(['bidder_id', 'device']).size()\n",
    "    device_counts = device_counts.reset_index(name='device_counts')\n",
    "    most_used_devices = device_counts.loc[device_counts.groupby('bidder_id')['device_counts'].idxmax()]\n",
    "    most_used_devices = most_used_devices.rename(columns={\"device\": \"most_used_device\"}).reset_index(drop=True)\n",
    "\n",
    "    country_counts = trade_df.groupby(['bidder_id', 'country']).size()\n",
    "    country_counts = country_counts.reset_index(name='country_counts')\n",
    "    most_used_countries = country_counts.loc[country_counts.groupby('bidder_id')['country_counts'].idxmax()]\n",
    "    most_used_countries = most_used_countries.rename(columns={\"country\": \"most_used_country\"}).reset_index(drop=True)\n",
    "\n",
    "    merchandise_counts = trade_df.groupby(['bidder_id', 'merchandise']).size()\n",
    "    merchandise_counts = merchandise_counts.reset_index(name='merchandise_counts')\n",
    "    most_merchandise = merchandise_counts.loc[merchandise_counts.groupby('bidder_id')['merchandise_counts'].idxmax()]\n",
    "    most_merchandise = most_merchandise.rename(columns={\"merchandise\":\"most_merchandise\"}).reset_index(drop=True)\n",
    "    \n",
    "    ip_counts = trade_df.groupby(['bidder_id', 'ip']).size()\n",
    "    ip_counts = ip_counts.reset_index(name='ip_counts')\n",
    "    most_used_ip = ip_counts.loc[ip_counts.groupby('bidder_id')['ip_counts'].idxmax()]\n",
    "    most_used_ip = most_used_ip.rename(columns={\"ip\":\"most_used_ip\"}).reset_index(drop=True)\n",
    "     \n",
    "    url_counts = trade_df.groupby(['bidder_id', 'url']).size()\n",
    "    url_counts = url_counts.reset_index(name='url_counts')\n",
    "    most_used_url = url_counts.loc[url_counts.groupby('bidder_id')['url_counts'].idxmax()]\n",
    "    most_used_url = most_used_url.rename(columns={\"url\":\"most_used_url\"}).reset_index(drop=True)\n",
    "\n",
    "    average_times = trade_df.groupby('bidder_id')['time_normalized'].mean()\n",
    "    average_times = average_times.reset_index(name='average_times')\n",
    "\n",
    "    feature_df = trade_count.merge(most_used_devices, on='bidder_id', how='left').reset_index(drop=True)\n",
    "    feature_df = feature_df.merge(most_used_countries, on='bidder_id', how='left').reset_index(drop=True)\n",
    "    feature_df = feature_df.merge(average_times, on='bidder_id', how='left').reset_index(drop=True)\n",
    "    feature_df = feature_df.merge(most_merchandise, on='bidder_id', how='left').reset_index(drop=True)\n",
    "    feature_df = feature_df.merge(most_used_ip, on='bidder_id', how='left').reset_index(drop=True)\n",
    "    feature_df = feature_df.merge(most_used_url, on='bidder_id', how='left').reset_index(drop=True)\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "user_feature_df = trade_data_processing(trade_df)\n",
    "train_df = train_df.merge(user_feature_df, on='bidder_id', how='left')\n",
    "test_df = test_df.merge(user_feature_df, on='bidder_id', how='left')\n",
    "\n",
    "def fillNAN(df):\n",
    "    num_columns = df.select_dtypes(include=[\"number\", \"bool\"]).columns\n",
    "    cat_columns = df.select_dtypes(exclude=[\"number\", \"bool\"]).columns\n",
    "\n",
    "    for col in num_columns:\n",
    "        mean_value = df[col].mean()\n",
    "        df[col].fillna(mean_value, inplace=True)\n",
    "\n",
    "    for col in cat_columns:\n",
    "        mode_value = df[col].mode()[0]\n",
    "        df[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = fillNAN(train_df)\n",
    "test_df = fillNAN(test_df)\n",
    "\n",
    "def remove_outliers_box(df):\n",
    "    cleaned_df = df.copy()  \n",
    "\n",
    "    # 处理连续型特征\n",
    "    #continuous_features = df.drop(columns=['outcome']).select_dtypes(include=['number']).columns\n",
    "    连续型特征 = df.drop(columns=['outcome']).select_dtypes(include=['number']).columns\n",
    "    for column in 连续型特征:\n",
    "        # 使用箱线图计算上下边缘\n",
    "        q1 = df[column].quantile(0.25)\n",
    "        q3 = df[column].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - 3 * iqr\n",
    "        upper = q3 + 3 * iqr\n",
    "\n",
    "        # 去除异常值\n",
    "        df[column] = np.where(df[column] > upper, upper, df[column])\n",
    "        df[column] = np.where(df[column] < lower, lower, df[column])\n",
    "\n",
    "    # 处理标称特征（如果需要的话）\n",
    "\n",
    "    return cleaned_df\n",
    "train_df = remove_outliers_box(train_df)\n",
    "\n",
    "# 存储卡方检验结果的字典\n",
    "chi2_results = {}\n",
    "\n",
    "# 选择标称属性列\n",
    "categorical_columns = ['most_used_device', 'most_used_country', 'most_merchandise','most_used_ip','most_used_url']\n",
    "\n",
    "# 对每个标称属性进行卡方检验\n",
    "for column in categorical_columns:  # 忽略最后一列标签列\n",
    "    contingency_table = pd.crosstab(train_df[column], train_df['outcome'])\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    chi2_results[column] = p\n",
    "\n",
    "# 将结果转换为 DataFrame 并按 p 值排序\n",
    "chi2_df = pd.DataFrame(list(chi2_results.items()), columns=['Feature', 'p_value'])\n",
    "chi2_df = chi2_df.sort_values(by='p_value')\n",
    "\n",
    "# 输出结果\n",
    "print(\"卡方检验结果 (按 p 值排序):\")\n",
    "for index, row in chi2_df.iterrows():\n",
    "    print(f\"{row['Feature']} 与标签的p值: {row['p_value']}\")\n",
    "\n",
    "\n",
    "# 判断并输出有显著相关性的特征\n",
    "alpha = 0.05\n",
    "significant_features = [feature for feature, p_value in chi2_results.items() if p_value < alpha]\n",
    "\n",
    "if significant_features:\n",
    "    print(\"\\n与标签有显著相关性的标称属性有：\")\n",
    "    for feature in significant_features:\n",
    "        print(feature)\n",
    "else:\n",
    "    print(\"\\n没有标称属性与标签之间有显著的相关性。\")\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# 选择数值型和布尔型数据\n",
    "num_features = train_df.select_dtypes(include=[\"number\", \"bool\"]).copy()\n",
    "num_features = num_features.apply(lambda x: int(x) if isinstance(x, bool) else x)\n",
    "# 计算相关系数矩阵\n",
    "corrmat = num_features.corr()\n",
    "\n",
    "# 选择与目标变量 'outcome' 相关性最大的5个属性\n",
    "k = 8\n",
    "cols = corrmat.nlargest(k + 1, 'outcome')['outcome'].index  # 加1是因为包括了目标变量本身\n",
    "cm = np.corrcoef(train_df[cols].values.T)\n",
    "\n",
    "# 画出heatmap\n",
    "sns.set_context(\"notebook\", font_scale=0.75)\n",
    "plt.figure(figsize=(6, 4))\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n",
    "                 annot_kws={'size': 8}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()\n",
    "\n",
    "# 打印相关性最大的5个属性\n",
    "print(\"与标签 'outcome' 相关性最大的8个属性：\")\n",
    "print(cols[1:])\n",
    "\n",
    "# 合并训练集和测试集\n",
    "combined_df = pd.concat([train_df, test_df], keys=['train', 'test'])\n",
    "# 进行独热编码\n",
    "# 'most_used_device', 'most_used_ip','most_used_country','most_merchandise','most_used_url'\n",
    "categorical_features = ['most_used_country','most_used_url']\n",
    "combined_encoded = pd.get_dummies(combined_df, columns=categorical_features)\n",
    "\n",
    "# 拆分回训练集和测试集\n",
    "train_encoded = combined_encoded.xs('train')\n",
    "test_encoded = combined_encoded.xs('test')\n",
    "train_encoded\n",
    "\n",
    "\n",
    "for column in train_df.columns:\n",
    "    if train_df[column].dtype == bool:\n",
    "        train_df[column] = train_df[column].astype(np.float64)\n",
    "for column in test_df.columns:\n",
    "    if test_df[column].dtype == bool:\n",
    "        test_df[column] = test_df[column].astype(np.float64)\n",
    "\n",
    "#删去非数值属性的列用于训练\n",
    "columns1 = train_encoded.select_dtypes(exclude=[\"number\", \"bool\"]).columns\n",
    "train = train_encoded.drop(columns=columns1)\n",
    "test = test_encoded.drop(columns=columns1)\n",
    "test = test.drop(columns=['outcome'])\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 定义要调优的参数网格\n",
    "param_grid1 = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_features': [2, 3, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# 创建随机森林分类器\n",
    "clf1 = RandomForestClassifier()\n",
    "\n",
    "# 实例化 GridSearchCV 对象\n",
    "grid_search1 = GridSearchCV(clf1, param_grid1, cv=5)\n",
    "\n",
    "# 使用网格搜索对模型进行参数调优\n",
    "grid_search1.fit(X, Y)\n",
    "\n",
    "# 输出最佳参数组合和对应的评分\n",
    "print(\"最佳参数组合: \", grid_search1.best_params_)\n",
    "print(\"最佳准确率: \", grid_search1.best_score_)\n",
    "\n",
    "# 使用最佳参数训练模型并进行评估\n",
    "best_clf1 = grid_search1.best_estimator_\n",
    "scores = cross_val_score(best_clf1, X, Y, cv=5)\n",
    "print('调优后RF准确率：', scores.mean())\n",
    "\n",
    "# 将train_df划分为数据集和训练集来训练模型\n",
    "train_df_train, train_df_test = train_test_split(train, test_size=0.3, random_state=0)\n",
    "y_real = train_df_test['outcome']\n",
    "X_train_df_test = train_df_test.drop(columns=['outcome']).select_dtypes(include=[\"number\", \"bool\"])\n",
    "selected_columns_df = X_train_df_test.loc[:, ['trade_count','average_times']]\n",
    "# 预测结果\n",
    "y_pred1 = best_clf1.predict(X_train_df_test)\n",
    "# 计算AUC\n",
    "auc1 = roc_auc_score(y_real, y_pred1)\n",
    "print(\"Valid aUC1: \", auc1)\n",
    "# 预测test_df数据集\n",
    "X_test = test.select_dtypes(include=[\"number\", \"bool\"])\n",
    "y_test1 = best_clf1.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "# 保存预测结果\n",
    "test_df['prediction'] = y_test1\n",
    "test_df[['prediction']].to_csv('result.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
